{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will see how we can use Bayesian optimization [[Shahriari et al, 2016]](#4.-References) to optimize the hyperparameters of a feed-forward neural network (FC-Net).\n",
    "We will also learn how we can empiricially evaluate hyperparameter optimization methods in practice.\n",
    "\n",
    "Before you start, make sure you installed the following dependencies:\n",
    "- numpy (pip install numpy)\n",
    "- matplotlib (pip install matplotlib)\n",
    "- scipy (pip install scipy)\n",
    "- sklearn (pip install sklearn)\n",
    "- pytorch (pip install torch)\n",
    "- ConfigSpace (pip install ConfigSpace)\n",
    "- hpobench (see https://github.com/automl/nas_benchmarks for how to install it. Note that, you need to download the data first from [here](http://ml4aad.org/wp-content/uploads/2019/01/fcnet_tabular_benchmarks.tar.gz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid, that you waste too much time training the FC-Net, we will not optimize the actual benchmark but instead use a  tabular benchmark [[Ying et al, 2019]](#4.-References) [[Klein et al, 2019]](#4.-References). In previous work, we first discretized the hyperparameter configuration space of the FC-Net and then performed an evaluated all hyperparameter configurations in that space multiple time. \n",
    "The results are compiled into a database, such that we can simply look up the performance of a hyperparameter configuration instead of training it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how we can load the benchmark together with the configuration space.\n",
    "Each hyperparameter configuration is encoded as a `Configuration` object and we can easily convert it to a `numpy` array or a `dictionary`. Note that, if we convert it to a numpy array, all values are normalized to be in $[0, 1]$.\n",
    "\n",
    "If we evaluate a hyperparameter configuration, we get the validation error and the time it had taken to train this configuration. When we generated this benchmark, we evaluated each hyperparameter configuration 4 times, and, for every table lookup, we pick one of these 4 trials uniformly at random. This simulates the typical noise that comes with hyperparameter optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_benchmarks import FCNetProteinStructureBenchmark\n",
    "from tabular_benchmarks.fcnet_benchmark import FCNetBenchmark\n",
    "\n",
    "b = FCNetProteinStructureBenchmark(data_dir=\"PATH_TO_FCNET_DATA\")\n",
    "cs = b.get_configuration_space()\n",
    "config = cs.sample_configuration()\n",
    "\n",
    "print(\"Numpy representation: \", config.get_array())\n",
    "print(\"Dict representation: \", config.get_dictionary())\n",
    "\n",
    "y, cost = b.objective_function(config)\n",
    "print(\"Validation error: %f\" % y)\n",
    "print(\"Runtime %f\" % cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will first implement random search [[Bergstra et al, 2012]](#4.-References), which is, besides its simplicity, usually a quite tough baseline to beat. If you develop a new method, you should always first compare against random search to see whether it actually works or if there is still a bug somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function draws `n_iters` hyperparameter configurations uniformly at random from the configuration space and keeps track of the incumbent (i.e the best configuration we have seen so far) after each function evaluation. Since in this exercise we are also interested in comparing different hyperparameter optimization methods, we return the incumbent after each time steps together with its validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(benchmark: FCNetBenchmark, n_iters: int = 100):\n",
    "    \n",
    "    # get the configuration space\n",
    "    cs = benchmark.get_configuration_space()\n",
    "    \n",
    "    incumbent = None  # the best observed configuration, might need to be updated after each function evaluation\n",
    "    incumbent_val = np.inf\n",
    "    \n",
    "    # some bookkeeping\n",
    "    runtime = []  # cumuliatve cost of the time we spend for function evaluations\n",
    "    incumbent_trajectory = []  # the incumbent after each function evaluation\n",
    "    incumbent_trajectory_error = []  # the corresponding validation error\n",
    "    \n",
    "    # start the random search loop\n",
    "    for i in range(n_iters):\n",
    "        \n",
    "        #TODO: sample hyperparameter configuration\n",
    "        \n",
    "        #TODO: evaluate it\n",
    "        \n",
    "        #TODO: check whether we improved upon the current incumbent\n",
    "            \n",
    "        #TODO: updated incumbent trajectory\n",
    "        \n",
    "    return incumbent_trajectory, incumbent_trajectory_error\n",
    "\n",
    "incumbent, traj = random_search(benchmark=b, n_iters=50)\n",
    "plt.plot(np.arange(1, 51), traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we we will implement Bayesian optimization [[Snoek et al, 2012]](#4.-References).\n",
    "As we saw in the lecture, Bayesian optimization has two main ingredients: the probablistic model and the acquisition function. Since we have a discrete space here, we will first use random forest [[Breimann et al, 2001]](#4.-References) to model the objective function instead of Gaussian processes which are the usually used for continuous spaces. For the acquisition function we will use expected improvement which is probably the most popular one in the literature. Additionaly, we also need an optimizer to maximize the acquisition function, and, due to the discrete space, we cannot use standard optimizer, such as for example scipy.optimize. Instead, we will implemented a simple stochastic local search method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the model. We will we write a wrapper around sklearn's random forest module, which returns for a given test point only the mean prediction. However, to compute the acquisition function, we also need the predictive variance. For that, we first loop over the trees to get the individual tree predictions and then compute the mean and variance of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class RandomForest(object):\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # TODO: Instantiate and train a random forest on the provided data\n",
    "        \n",
    "    def predict(self, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        #TODO: Loop over tree and compute the mean and variance over the tree predictions\n",
    "        \n",
    "        return mean, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement expected improvement. To compute the CDF and the PDF you can use the scipy functions: `scipy.norm.cdf` and `scipy.norm.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(candidates: np.ndarray, model, y_star: float) -> np.ndarray:\n",
    "    # TODO: compute the improvement for the candidate points over y_star in expectation based on the model's predictions\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned above, to optimize the acquisition function we will implement a simple local search method, which works as follows:\n",
    " 1. start from an initial point `x_init`\n",
    " 2. loop over all its one-step neighbours and compute the acquisition function values\n",
    " 3. jump to the neighbour with the highest acquisition function value\n",
    " 4. repeat step 2 and 3 until we either reach the maximum number of steps `n_steps` or we don't improve anymore\n",
    " 5. return best found configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace\n",
    "from ConfigSpace.util import get_one_exchange_neighbourhood  # see docstring: https://github.com/automl/ConfigSpace/blob/master/ConfigSpace/util.pyx for more details\n",
    "\n",
    "def local_search(acquisition_function: func, model: Model, y_star: float,\n",
    "                 x_init: ConfigSpace.Configuration, n_steps: int) -> ConfigSpace.Configuration:\n",
    "    current_best = x_init\n",
    "    current_best_value = f(x_init.get_array()[None, :])\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # TODO: evaluate one-step neighbourhood (hint: use get_one_exchange_neighbourhood function)\n",
    "        \n",
    "        # TODO: check whether we improved upon the current best\n",
    "        \n",
    "        # TODO: jump to the next neighbour if we improved\n",
    "        \n",
    "        # TODO: in case we converged, stop the local search\n",
    "        \n",
    "    return current_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our ingredients together, and we can finally implement the main Bayesian optimization loop.\n",
    "Before we can fit a model, we need to collect some data first. This is called the initial design.\n",
    "Various different initial design strategies exist, but here we will simply sample `n_init` random points.\n",
    "\n",
    "Note that, like for random search, we want to benchmark Bayesian optimization later and need the performance of the incumbent over time. Make sure that you keep track of the incumbent and check after *each function evaluation* whether we improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def bayesian_optimization(benchmark: FCNetBenchmark, model, acquisition_function=expected_improvement,\n",
    "                          optimizer=local_search, n_iters: int = 100, n_init: int = 5) -> None:\n",
    "    # book keeping\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    incumbent_trajectory = []\n",
    "    incumbent_trajectory_error = []\n",
    "    incumbent_val = np.inf\n",
    "    incumbent = None\n",
    "    \n",
    "    # TODO: implement initial design by evaluating random configurations\n",
    "    for i in range(n_init):\n",
    "        \n",
    "    # start main BO loop\n",
    "    for i in range(n_init, n_iters):\n",
    "        \n",
    "        # TODO: fit model\n",
    "        \n",
    "        # TODO: optimize acquisition function to get candidate point\n",
    "        \n",
    "        # TODO: evaluate objective function at the candidate point\n",
    "        \n",
    "        # TODO: book keeping\n",
    "        \n",
    "    return incumbent_trajectory, incumbent_trajectory_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incumbent, traj = bayesian_optimization(b, RandomForest, n_iters=50)\n",
    "plt.plot(np.arange(1, 51), traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional model we will try DNGO [[Snoek et al, 2015]](#4.-References), which first fits a neural networks with a linear output layer.\n",
    "After training, it chops off the output layer and uses Bayesian linear regression with the output of the last layer as basis functions.\n",
    "During inference time, it pass the test data through the first layers and then uses the precomputed Basis linear regression terms (m, K) to compute the mean and the variance. For more details have a look in Section 3 in the paper by [[Snoek et al, 2015]](#4.-References)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_inputs, n_units=50):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_units)\n",
    "        self.fc2 = nn.Linear(n_units, n_units)\n",
    "        self.fc3 = nn.Linear(n_units, n_units)\n",
    "        self.out = nn.Linear(n_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "    def basis_funcs(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class DNGO(object):\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, num_epochs: int = 200) -> None:\n",
    "        \n",
    "        # TODO: create the neural network\n",
    "        \n",
    "        # TODO: implement the training loop\n",
    "                \n",
    "        # Hyperparameters for the Bayesian linear regression.\n",
    "        # Note: in the paper they sample \\alpha an \\beta from the marginal log-likelihood \n",
    "        # However, for simplicity, we keep them fix\n",
    "        self.alpha = 1E-3\n",
    "        self.beta = 1000\n",
    "\n",
    "        # TODO: compute the Bayesian linear regression terms\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        # TODO: extract the basis functions\n",
    "        \n",
    "        # TODO: compute the mean and the variance based on the Bayesian linear regression terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major part in developing new optimizers is the empirical comparison to existing baselines.\n",
    "Even though this is often cumbersome and frustrating, it is key to obtain a better understanding and hence to develop better methods.\n",
    "\n",
    "Since the most algorithms are to a certain degree randomized, we need to perform independent runs of each method with a different intialization in order to draw statistical significant conclusions.\n",
    "In practice, this is often problematic due to the high computational cost of the most hyperparameter optimization problems. With our tabular benchmarks however, we can easiliy afford multiple runs and are only limited by the optimizer's overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we run each methods `30` times for `50` iterations and store the incumbent trajectories of each run. If we have parallel resources available, such as for instance with a compute cluster, it is good practice to parallelize everything as much as possible ot get the most efficient workload. However, since we compare only 3 methods for a moderate number of runs and iterations, we can afford it to run them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_runs = 30\n",
    "n_iters = 50\n",
    "rs_results = np.zeros([n_runs, n_iters])\n",
    "bo_rf_results = np.zeros([n_runs, n_iters])\n",
    "bo_dngo_results = np.zeros([n_runs, n_iters])\n",
    "\n",
    "rs_incumbents = []\n",
    "bo_rf_incumbents = []\n",
    "bo_dngo_incumbents = []\n",
    "\n",
    "# TODO: evaluate each methods 'n_runs' times for 'n_iters' iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice property of the tabular benchmarks is that we know the true global optimum (in terms of test error averaged over the four trials).\n",
    "To estimate an optimzer's quality, a popular metric is the regret: $|y_{incumbent} - y_{\\star}|$ which measures the difference between the performance $y_{\\star}$ of the global optimum and the current incumbent performance $y_{incumbent}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_star_valid, y_star_test = b.get_best_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a robust estimated of an optimzer's performance, we first compute the regret and then plot the incumbent trajectory across all independent runs of each optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot mean validation regret of each methods averaged over all runs\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"validation regret\")\n",
    "plt.xlabel(\"number of function evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we are eventually interested in the test performance rather than the validation performance. Thus, it is also good practice to perform an offline evaluation to compute the test performance of all incumbents.\n",
    "The tabular benchmarks allow us to do that efficiently and, as for the validation performance, we can again compute the regret to the true performance (y_star_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute test performance for all methods and all runs and plot the average test regret\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"test regret\")\n",
    "plt.xlabel(\"number of function evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L. Breimann (2001) *Random Forests*  Machine Learning\n",
    "* J. Bergstra and Y. Bengio (2012) *Random Search for Hyper-Parameter Optimization* Journal of Machine Learning Research \n",
    "\n",
    "* B. Shahriari and K. Swersky and Z. Wang and R. Adams and N. de Freitas (2016), *Taking the Human Out of the Loop: {A} Review of {B}ayesian Optimization* Proceedings of the {IEEE}\n",
    "  \n",
    "* J. Snoek and H. Larochelle and R. P. Adams (2012) *Practical {B}ayesian Optimization of Machine Learning Algorithms* Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'12)\n",
    "\n",
    "* J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams (2015) *Scalable {B}ayesian Optimization Using Deep Neural Networks* Proceedings of the 32nd International Conference on Machine Learning (ICML'15)\n",
    "        \n",
    "* C. Ying and A. Klein and E. Real and E. Christiansen and K. Murphy and F. Hutter (2019) *NAS-Bench-101: Towards Reproducible Neural Architecture Search* arXiv:1902.09635\n",
    "\n",
    "* A. Klein and F. Hutter (2019) *Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization* arXiv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
